{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7607183",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search for Classifier Chains\n",
    "\n",
    "This notebook gives an example of how to use the framework built in this repository. Feel free to play around with the different parameters of the notebook, and make use of other policies implemented in the [policy.py](../src/mcts_inference/policy.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 8\n",
    "n_labels = 2\n",
    "random_state = 0\n",
    "\n",
    "X, Y = make_multilabel_classification(\n",
    "    n_samples = n_samples,\n",
    "    n_features = n_features,\n",
    "    n_classes = n_classes,\n",
    "    n_labels = n_labels,\n",
    "    random_state = random_state)\n",
    "\n",
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "solver = \"liblinear\"\n",
    "base = LogisticRegression(solver=solver)\n",
    "chain = ClassifierChain(base)\n",
    "\n",
    "chain = chain.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f25990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts_inference.mcts import MCTS\n",
    "from mcts_inference.mcts_config import MCTSConfig\n",
    "from mcts_inference.constraints import Constraint\n",
    "from mcts_inference.policy import UCB \n",
    "\n",
    "constraint = Constraint(max_iter=True, n_iter=1000)\n",
    "config1 = MCTSConfig(n_classes, selection_policy=UCB(), constraint=constraint, verbose=True, step_once=True)\n",
    "config2 = MCTSConfig(n_classes                        , constraint=constraint, verbose=True, step_once=True)\n",
    "\n",
    "M: int = 100\n",
    "out1 = MCTS(X_test[:M], chain, config1)\n",
    "out2 = MCTS(X_test[:M], chain, config2)\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "print(\"Hamming loss of chain              : \", hamming_loss(chain.predict(X_test[:M]), Y_test[:M]))\n",
    "print(\"Hamming loss of MCTS UCB           : \", hamming_loss(out1, Y_test[:M]))\n",
    "print(\"Hamming loss of MCTS Epsilon Greedy: \", hamming_loss(out2, Y_test[:M]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed725e7",
   "metadata": {},
   "source": [
    "## Iteration to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.mcts_inference.mcts import MCTS\n",
    "from src.mcts_inference.mcts_config import MCTSConfig\n",
    "from src.mcts_inference.constraints import Constraint\n",
    "from src.mcts_inference.policy import UCB \n",
    "\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "solver = \"liblinear\"\n",
    "\n",
    "import time\n",
    "\n",
    "Ns = [100, 500, 1000, 1500, 2000]\n",
    "Ls = [2, 4, 8, 16, 32, 64]\n",
    "avg = 10\n",
    "M = 100\n",
    "\n",
    "n_samples = 10000\n",
    "n_labels = 2\n",
    "random_state = 0\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [[[] for _ in Ns] for _ in Ls]\n",
    "\n",
    "for il, l in enumerate(Ls):\n",
    "    t = time.time()\n",
    "    print(\"L>\", l)\n",
    "    X, Y = make_multilabel_classification(\n",
    "        n_samples = n_samples,\n",
    "        n_features = 4 * l,\n",
    "        n_classes = l,\n",
    "        n_labels = n_labels,\n",
    "        random_state = random_state)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    base = LogisticRegression(solver=solver)\n",
    "    chain = ClassifierChain(base)\n",
    "\n",
    "    chain = chain.fit(X_train,Y_train)\n",
    "    \n",
    "    print(\">n:\", end=\" \")\n",
    "    for jn, n in enumerate(Ns):\n",
    "        tt = time.time()\n",
    "        print(n, end=\", \")\n",
    "        constraint = Constraint(max_iter=True, n_iter=n)\n",
    "        config = MCTSConfig(l, selection_policy=UCB(), constraint=constraint, verbose=False, step_once=False, parallel=True)\n",
    "        for _ in range(avg):\n",
    "            t0 = time.time()\n",
    "\n",
    "            MCTS(X_test[:M], chain, config)\n",
    "\n",
    "            times[il][jn].append(time.time()-t0)\n",
    "        print(f\"({time.time()-tt:.2f}s)\", end = \" \")\n",
    "    print(f\"\\n({time.time()-t:.3f}s)\")\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70540e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(np.mean(times,axis=-1))  # Store z, took 40 minutes to run.\n",
    "# array([[ 0.07813816,  0.16268473,  0.26474874,  0.36404204,  0.51120052],\n",
    "#        [ 0.11869841,  0.3139014 ,  0.534267  ,  0.74213264,  0.95371227],\n",
    "#        [ 0.41399403,  1.13965526,  1.56720562,  2.00165966,  2.41907544],\n",
    "#        [ 0.88097756,  3.77394373,  7.51101863, 11.32408068, 15.06112485],\n",
    "#        [ 1.58284893,  7.38303893, 14.78826919, 22.28634632, 29.75484819],\n",
    "#        [ 3.07422526, 14.72650027, 29.62631023, 44.4082509 , 59.50828764]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in Ls:\n",
    "    t = time.time()\n",
    "    print(\"L>\", l)\n",
    "    print(\"  n:\", end=\" \")\n",
    "    for n in Ns:\n",
    "        tt = time.time()\n",
    "        print(n, end=\", \")\n",
    "        time.sleep(0.2)\n",
    "        print(f\"({time.time()-tt:.2f}s)\", end=\" \")\n",
    "    print(f\"({time.time()-t:.3f}s)\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d36b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x, y = np.meshgrid(Ns, Ls)\n",
    "z = np.array(np.mean(times,axis=-1))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_wireframe(x, y, z, rstride=1, cstride=1, linewidth=1, alpha=1., label=\"Time\")\n",
    "surf = ax.plot_wireframe(x, y, np.log(z), rstride=1, cstride=1, linewidth=1, alpha=0.5, label=\"Log Time\")\n",
    "\n",
    "\n",
    "plt.xticks(Ns, fontsize=8)\n",
    "plt.yticks(Ls, fontsize=8)\n",
    "ax.set_xlabel('Number of Iterations',fontsize=10)\n",
    "ax.set_ylabel('Number of Labels',fontsize=10)\n",
    "ax.set_zlabel('Time (s)',fontdict={'fontsize': 10})\n",
    "\n",
    "fig.colorbar(surf,shrink=0., aspect=1)\n",
    "\n",
    "plt.legend(loc=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zz, ll in zip(z, Ls):\n",
    "    plt.plot(Ns,zz,marker=\"o\",label=f\"L={ll}\",markersize=2)\n",
    "plt.xticks(Ns)\n",
    "plt.legend()\n",
    "plt.title(\"Time (s) vs Number of Iterations\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef690c6",
   "metadata": {},
   "source": [
    "## MCTS with few Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from mcts_inference.mcts import MCTS\n",
    "from mcts_inference.mcts_config import MCTSConfig\n",
    "from mcts_inference.policy import UCB\n",
    "from mcts_inference.constraints import Constraint\n",
    "\n",
    "Ns = [x for x in range(100, 2000, 50)]\n",
    "M = 100\n",
    "\n",
    "losses4 = []\n",
    "chain_losses: float = hamming_loss(chain.predict(X_test[:M]), Y_test[:M])\n",
    "\n",
    "print(\"> n: \",end=\"\")\n",
    "for n in Ns:\n",
    "    print(n, end=\", \")\n",
    "    constraint = Constraint(max_iter=True, n_iter=n)\n",
    "\n",
    "    config4 = MCTSConfig(n_classes, exploration_policy=UCB(), constraint=constraint, step_once=True)\n",
    "\n",
    "    avg = 10\n",
    "    loss = 0\n",
    "    for i in range(avg):\n",
    "        out4 = MCTS(X_test[:M], chain, config4)\n",
    "        loss += hamming_loss(out4, Y_test[:M])\n",
    "\n",
    "    losses4.append(loss/avg)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(Ns, losses4, label=\"MCTS\", marker=\"o\", markersize=3)\n",
    "plt.plot(Ns, [chain_losses]*len(Ns), label=\"Chain\", marker=\"o\", markersize=1)\n",
    "\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Hamming Loss\")\n",
    "\n",
    "plt.title(f\"Hamming Loss vs Number of Iterations, L={n_classes}, Averaged over {avg} runs\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
