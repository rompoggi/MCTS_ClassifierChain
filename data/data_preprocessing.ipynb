{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212749e6",
   "metadata": {},
   "source": [
    "# Cleaning up of datasets for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8600337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files = True\n",
    "\n",
    "if save_files:\n",
    "    import os\n",
    "    if (os.path.exists(\"./data/datasets\") is False):\n",
    "        os.makedirs(\"./data/datasets\")\n",
    "        \n",
    "    assert(os.path.exists(\"./data/datasets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15734c",
   "metadata": {},
   "source": [
    "## 1. Solar Flare: N = 1389, L = 3, d = 10, K=5: cannot be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb99457e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'uci_id': 89, 'name': 'Solar Flare', 'repository_url': 'https://archive.ics.uci.edu/dataset/89/solar+flare', 'data_url': 'https://archive.ics.uci.edu/static/public/89/data.csv', 'abstract': 'Each class attribute counts the number of solar flares of a certain class that occur in a 24 hour period', 'area': 'Physics and Chemistry', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1389, 'num_features': 10, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['common flares', 'moderate flares', 'severe flares'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Wed Feb 14 2024', 'dataset_doi': '10.24432/C5530G', 'creators': [], 'intro_paper': None, 'additional_info': {'summary': 'Notes:\\r\\n\\r\\n   -- The database contains 3 potential classes, one for the number of times a certain type of solar flare occured in a 24 hour period.\\r\\n   -- Each instance represents captured features for 1 active region on the sun.\\r\\n   -- The data are divided into two sections. The second section (flare.data2) has had much more error correction applied to the it, and has consequently been treated as more reliable.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"   1. Code for class (modified Zurich class)  (A,B,C,D,E,F,H)\\r\\n   2. Code for largest spot size              (X,R,S,A,H,K)\\r\\n   3. Code for spot distribution              (X,O,I,C)\\r\\n   4. Activity                                (1 = reduced, 2 = unchanged)\\r\\n   5. Evolution                               (1 = decay, 2 = no growth, 3 = growth)\\r\\n   6. Previous 24 hour flare activity code    (1 = nothing as big as an M1, 2 = one M1, 3 = more activity than one M1)\\r\\n   7. Historically-complex                    (1 = Yes, 2 = No)\\r\\n   8. Did region become historically complex  on this pass across the sun's disk (1 = yes, 2 = no) \\r\\n   9. Area                                    (1 = small, 2 = large)\\r\\n  10. Area of the largest spot                (1 = <=5, 2 = >5)\\r\\n\\r\\n From all these predictors three classes of flares are predicted, which are represented in the last three columns.\\r\\n\\r\\n  11. C-class flares production by this region in the following 24 hours (common flares); Number\\r\\n  12. M-class flares production by this region in the following 24 hours (moderate flares);   Number\\r\\n  13. X-class flares production by this region in the following 24 hours (severe flares); Number\\r\\n     \", 'citation': None}}\n",
      "                               name     role         type demographic  \\\n",
      "0             modified Zurich class  Feature  Categorical        None   \n",
      "1                 largest spot size  Feature  Categorical        None   \n",
      "2                 spot distribution  Feature  Categorical        None   \n",
      "3                          activity  Feature      Integer        None   \n",
      "4                         evolution  Feature      Integer        None   \n",
      "5   previous 24 hour flare activity  Feature      Integer        None   \n",
      "6              historically-complex  Feature      Integer        None   \n",
      "7       became complex on this pass  Feature      Integer        None   \n",
      "8                              area  Feature      Integer        None   \n",
      "9              area of largest spot  Feature      Integer        None   \n",
      "10                    common flares   Target      Integer        None   \n",
      "11                  moderate flares   Target      Integer        None   \n",
      "12                    severe flares   Target      Integer        None   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                       A,B,C,D,E,F,H  None             no  \n",
      "1                                         X,R,S,A,H,K  None             no  \n",
      "2                                             X,O,I,C  None             no  \n",
      "3                          1 = reduced, 2 = unchanged  None             no  \n",
      "4                1 = decay, 2 = no growth, 3 = growth  None             no  \n",
      "5   1 = nothing as big as an M1, 2 = one M1, 3 = m...  None             no  \n",
      "6                                     1 = Yes, 2 = No  None             no  \n",
      "7   . Did region become historically complex  on t...  None             no  \n",
      "8                                1 = small, 2 = large  None             no  \n",
      "9                                     1 = <=5, 2 = >5  None             no  \n",
      "10  C-class flares production by this region in th...  None             no  \n",
      "11  M-class flares production by this region in th...  None             no  \n",
      "12  X-class flares production by this region in th...  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset \n",
    "solar_flare = fetch_ucirepo(name=\"Solar Flare\") \n",
    "\n",
    "print(solar_flare.data.version2)\n",
    "  \n",
    "# data (as pandas dataframes)\n",
    "X = solar_flare.data.features \n",
    "y = solar_flare.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(solar_flare.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(solar_flare.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967601e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      modified Zurich class  largest spot size  spot distribution  activity  \\\n",
       " 0                         1                  4                  2         1   \n",
       " 1                         2                  4                  2         1   \n",
       " 2                         1                  4                  2         1   \n",
       " 3                         2                  4                  2         1   \n",
       " 4                         2                  0                  2         1   \n",
       " ...                     ...                ...                ...       ...   \n",
       " 1384                      5                  4                  3         1   \n",
       " 1385                      5                  4                  3         2   \n",
       " 1386                      1                  4                  2         1   \n",
       " 1387                      5                  3                  3         1   \n",
       " 1388                      0                  5                  2         1   \n",
       " \n",
       "       evolution  previous 24 hour flare activity  historically-complex  \\\n",
       " 0             2                                1                     1   \n",
       " 1             3                                1                     1   \n",
       " 2             3                                1                     1   \n",
       " 3             3                                1                     1   \n",
       " 4             3                                1                     1   \n",
       " ...         ...                              ...                   ...   \n",
       " 1384          2                                1                     1   \n",
       " 1385          2                                1                     1   \n",
       " 1386          2                                1                     2   \n",
       " 1387          2                                1                     1   \n",
       " 1388          1                                1                     1   \n",
       " \n",
       "       became complex on this pass  area  area of largest spot  \n",
       " 0                               2     1                     2  \n",
       " 1                               2     1                     2  \n",
       " 2                               2     1                     1  \n",
       " 3                               2     1                     2  \n",
       " 4                               2     1                     2  \n",
       " ...                           ...   ...                   ...  \n",
       " 1384                            1     1                     1  \n",
       " 1385                            2     1                     1  \n",
       " 1386                            2     1                     1  \n",
       " 1387                            2     1                     1  \n",
       " 1388                            2     1                     1  \n",
       " \n",
       " [1389 rows x 10 columns],\n",
       "       common flares  moderate flares  severe flares\n",
       " 0                 0                0              0\n",
       " 1                 0                0              0\n",
       " 2                 0                0              0\n",
       " 3                 0                0              0\n",
       " 4                 0                0              0\n",
       " ...             ...              ...            ...\n",
       " 1384              0                0              0\n",
       " 1385              0                0              0\n",
       " 1386              0                0              0\n",
       " 1387              0                0              0\n",
       " 1388              0                0              0\n",
       " \n",
       " [1389 rows x 3 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "clean_X = X.copy()\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(X[x])      \n",
    "\n",
    "X = clean_X.copy()\n",
    "del clean_X\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963ce7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Att1  Att2  Att3  Att4  Att5  Att6  Att7  Att8  Att9  Att10\n",
       " 0        1     4     2     1     2     1     1     2     1      2\n",
       " 1        2     4     2     1     3     1     1     2     1      2\n",
       " 2        1     4     2     1     3     1     1     2     1      1\n",
       " 3        2     4     2     1     3     1     1     2     1      2\n",
       " 4        2     0     2     1     3     1     1     2     1      2\n",
       " ...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...\n",
       " 1384     5     4     3     1     2     1     1     1     1      1\n",
       " 1385     5     4     3     2     2     1     1     2     1      1\n",
       " 1386     1     4     2     1     2     1     2     2     1      1\n",
       " 1387     5     3     3     1     2     1     1     2     1      1\n",
       " 1388     0     5     2     1     1     1     1     2     1      1\n",
       " \n",
       " [1389 rows x 10 columns],\n",
       "       Class1  Class2  Class3\n",
       " 0          0       0       0\n",
       " 1          0       0       0\n",
       " 2          0       0       0\n",
       " 3          0       0       0\n",
       " 4          0       0       0\n",
       " ...      ...     ...     ...\n",
       " 1384       0       0       0\n",
       " 1385       0       0       0\n",
       " 1386       0       0       0\n",
       " 1387       0       0       0\n",
       " 1388       0       0       0\n",
       " \n",
       " [1389 rows x 3 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3b312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files & False:\n",
    "    X.to_csv(\"./data/datasets/1-FLARE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/1-FLARE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be05f0",
   "metadata": {},
   "source": [
    "## 0.2. Bridges, N = 105, L = 6 , d = 7, not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce424d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bridges_raw_file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./raw_data/bridges.arff\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(bridges_raw_file_name))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load ARFF file\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data, meta \u001b[38;5;241m=\u001b[39m arff\u001b[38;5;241m.\u001b[39mloadarff(bridges_raw_file_name,)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bridges_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/bridges.arff\"\n",
    "assert(os.path.isfile(bridges_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(bridges_raw_file_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"TYPE\"])\n",
    "y = pd.DataFrame(df[\"TYPE\"])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        X[x] = label_encoder.fit_transform(X[x])\n",
    "\n",
    "y = pd.DataFrame(df[\"TYPE\"])\n",
    "y = pd.get_dummies(y[\"TYPE\"], prefix=\"TYPE\")\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d18b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:  # and False since we are not using bridges\n",
    "    X.to_csv(\"./data/datasets/2-BRIDGES_X.csv\")\n",
    "    y.to_csv(\"./data/datasets/2-BRIDGES_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6964c",
   "metadata": {},
   "source": [
    "## 0.3. Parkinson's: L = 1, Not usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "parkinsons = fetch_ucirepo(id=174) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = parkinsons.data.features \n",
    "y = parkinsons.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(parkinsons.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(parkinsons.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc48c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "clean_X = X.drop(columns=[\"MDVP:Jitter\", \"MDVP:Shimmer\"]).copy()\n",
    "clean_X[\"MDVP:Jitter\"] = X[\"MDVP:Jitter\"].values.T[0]\n",
    "clean_X[\"MDVP:Shimmer\"] = X[\"MDVP:Shimmer\"].values.T[0]\n",
    "\n",
    "\n",
    "for x in clean_X.columns:\n",
    "    if clean_X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(clean_X[x])\n",
    "\n",
    "clean_y = y.copy()\n",
    "for yc in clean_y.columns:\n",
    "    if clean_y[yc].dtype == 'object':\n",
    "        clean_y[yc] = label_encoder.fit_transform(clean_y[yc])\n",
    "        \n",
    "clean_X, clean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3379aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:  # and False since we are not using parkinsons\n",
    "    clean_X.to_csv(\"./data/3-PARKINS_X.csv\")\n",
    "    clean_y.to_csv(\"./data/3-PARKINS_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f6297",
   "metadata": {},
   "source": [
    "## 0.4. Thyroid: Missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert(os.path.isfile(bridges_raw_file_name))\n",
    "thyroid_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/hypothyroid.csv\"\n",
    "df = pd.read_csv(thyroid_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"binaryClass\"])\n",
    "y = pd.DataFrame(df[\"binaryClass\"])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X[\"TBG measured\"]), set(X[\"TBG\"])  # Can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e671317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=[\"TBG measured\", \"TBG\"])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340245bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e254f7",
   "metadata": {},
   "source": [
    "## 2. Music: 593, 6, 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "emot_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/2-EMOT.csv\"\n",
    "df = pd.read_csv(emot_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df.columns[:6]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=targets)\n",
    "y = df[targets]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/2-EMOT_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/2-EMOT_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dcefe",
   "metadata": {},
   "source": [
    "## 3. Scence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaace8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "scene_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/3-SCENE.csv\"\n",
    "df = pd.read_csv(scene_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f3e2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('Att')]\n",
    "\n",
    "X = df[columns]\n",
    "y = df.drop(columns=columns)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_cols = dict()\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/3-SCENE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/3-SCENE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69343255",
   "metadata": {},
   "source": [
    "## 4. Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "flags_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/flags.arff\"\n",
    "assert(os.path.isfile(flags_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(flags_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5471109",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"red\", \"green\", \"blue\", \"yellow\", 'white', 'black', \"orange\"]\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/4-FLAGS_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/4-FLAGS_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fe38c",
   "metadata": {},
   "source": [
    "# 5. Foodtruck: N = 407, L = 12, d = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "mediamill_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/foodtruck.arff\"\n",
    "assert(os.path.isfile(mediamill_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(mediamill_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['gourmet', 'snacks', 'street_food', 'italian_food',\n",
    "           'brazilian_food', 'mexican_food', 'chinese_food','japanese_food',\n",
    "           'arabic_food', 'healthy_food', 'fitness_food', 'sweets_desserts']\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X = X.copy()\n",
    "for x in clean_X.columns:\n",
    "    if clean_X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(clean_X[x])\n",
    "\n",
    "X = clean_X.copy()\n",
    "del clean_X\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/5-FOODTRUCK_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/5-FOODTRUCK_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff6e10",
   "metadata": {},
   "source": [
    "## 6. Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "yeast_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/yeast.arff\"\n",
    "assert(os.path.isfile(yeast_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(yeast_raw_file_name)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d593b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('Att')]\n",
    "\n",
    "X = df[columns]\n",
    "y = df.drop(columns=columns)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d11122",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/6-YEAST_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/6-YEAST_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5750c",
   "metadata": {},
   "source": [
    "## 7. Birds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04853e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "birds_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/7-BIRDS.csv\"\n",
    "df = pd.read_csv(birds_csv_file)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb84142",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[df.columns.str.startswith('A')]]  # Selecting columns with names starting with 'A'\n",
    "y = df[df.columns[df.columns.str.startswith('L')]]  # Selecting columns with names starting with 'L'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/7-BIRDS_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/7-BIRDS_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13134354",
   "metadata": {},
   "source": [
    "## 8. Genbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87187d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "genbase_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/genbase.arff\"\n",
    "assert(os.path.isfile(genbase_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(genbase_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba60513",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('PDOC')]\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        X[x] = label_encoder.fit_transform(X[x])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1631308",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/8-GENBASE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/8-GENBASE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1da308",
   "metadata": {},
   "source": [
    "## 9. Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "medc_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/9-MEDC.csv\"\n",
    "df = pd.read_csv(medc_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70336ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=df.columns[df.columns.str.startswith('Class')])  # Selecting columns with names starting with 'L'\n",
    "y = df[df.columns[df.columns.str.startswith('Class')]]  # Selecting columns with names starting with 'A'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/9-MEDC_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/9-MEDC_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b4110",
   "metadata": {},
   "source": [
    "## 10. Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enron_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/10-ENRON.csv\"\n",
    "df = pd.read_csv(enron_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708392f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in df.columns if col[0].isupper()]\n",
    "\n",
    "# Create a new DataFrame with the filtered columns\n",
    "X = df.drop(columns=filtered_columns)\n",
    "y = df[filtered_columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffa326",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/10-ENRON_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/10-ENRON_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012a13a",
   "metadata": {},
   "source": [
    "## 11. MediaMill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9293698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "mediamill_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/mediamill.arff\"\n",
    "assert(os.path.isfile(mediamill_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(mediamill_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34be7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[df.columns[df.columns.str.startswith('Att')]]  # Selecting columns with names starting with 'A'\n",
    "y = df[df.columns[df.columns.str.startswith('Class')]]  # Selecting columns with names starting with 'L'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b683b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/11-MEDIAMILL_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/11-MEDIAMILL_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bec85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
