{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212749e6",
   "metadata": {},
   "source": [
    "# Preparation of evaluation datasets\n",
    "\n",
    "In this notebook, we prepare the raw datasets stored in the [raw_datasets/](./raw_datasets/) directory for use in the [evaluation.ipynb](evaluation.ipynb) notebook. It assumes that the raw data exsits in the directory they should be part of. We include the pre processing of the following datasets:\n",
    "\n",
    "- 0.1. [Solar Flare](https://archive.ics.uci.edu/dataset/89/solar+flare)\n",
    "- 0.2. [Bridges](https://archive.ics.uci.edu/dataset/18/pittsburgh+bridges)\n",
    "- 0.3. [Parkinson's](https://www.kaggle.com/datasets/vikasukani/parkinsons-disease-data-set)\n",
    "- 0.4. [Thyroid](https://www.kaggle.com/datasets/yasserhessein/thyroid-disease-data-set/data)\n",
    "- 2 [Emotion](https://cometa.ujaen.es/datasets/emotions)\n",
    "- 3 [Scene](https://cometa.ujaen.es/datasets/scene)\n",
    "- 4 [Flags](https://cometa.ujaen.es/datasets/flags)\n",
    "- 5 [Foodtruck](https://cometa.ujaen.es/datasets/foodtruck)\n",
    "- 6 [Yeast](https://cometa.ujaen.es/datasets/yeast)\n",
    "- 7 [Birds](https://cometa.ujaen.es/datasets/birds)\n",
    "- 8 [Genbase](https://cometa.ujaen.es/datasets/genbase)\n",
    "- 9 [Medical](https://cometa.ujaen.es/datasets/medical)\n",
    "- 10 [Enron](https://cometa.ujaen.es/datasets/enron)\n",
    "- 11 [MediaMill](https://cometa.ujaen.es/datasets/mediamill)\n",
    "\n",
    "Note that the datasets with a leading 0 (namely the first 4) were not used in the evaluation because of missing data or one-dimensional classification tasks. Moreover, the last three datasets were not used in the final evaluation due to their considerable size, but could be used in future works to improve the results obtained. Once again, do not hesitate to contact me (Romain Poggi) at either **romain.poggi@polytechnique.edu** or **ropoggi323@gmail.com** if any issues were found in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files = True  # True if you want create/overwrite the already processed data.\n",
    "\n",
    "if save_files:\n",
    "    import os\n",
    "    if (os.path.exists(\"./data/datasets\") is False):\n",
    "        os.makedirs(\"./data/datasets\")\n",
    "        \n",
    "    assert(os.path.exists(\"./data/datasets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f15734c",
   "metadata": {},
   "source": [
    "## 0.1 Solar Flare: Not Used [Link](https://archive.ics.uci.edu/dataset/89/solar+flare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99457e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset \n",
    "solar_flare = fetch_ucirepo(name=\"Solar Flare\") \n",
    "\n",
    "print(solar_flare.data.version2)\n",
    "  \n",
    "# data (as pandas dataframes)\n",
    "X = solar_flare.data.features \n",
    "y = solar_flare.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(solar_flare.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(solar_flare.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967601e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "clean_X = X.copy()\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(X[x])      \n",
    "\n",
    "X = clean_X.copy()\n",
    "del clean_X\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ce7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files & False:\n",
    "    X.to_csv(\"./data/datasets/1-FLARE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/1-FLARE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be05f0",
   "metadata": {},
   "source": [
    "## 0.2. Bridges: Not Used [Link](https://archive.ics.uci.edu/dataset/18/pittsburgh+bridges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce424d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "bridges_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/bridges.arff\"\n",
    "assert(os.path.isfile(bridges_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(bridges_raw_file_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"TYPE\"])\n",
    "y = pd.DataFrame(df[\"TYPE\"])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        X[x] = label_encoder.fit_transform(X[x])\n",
    "\n",
    "y = pd.DataFrame(df[\"TYPE\"])\n",
    "y = pd.get_dummies(y[\"TYPE\"], prefix=\"TYPE\")\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d18b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:  # and False since we are not using bridges\n",
    "    X.to_csv(\"./data/datasets/2-BRIDGES_X.csv\")\n",
    "    y.to_csv(\"./data/datasets/2-BRIDGES_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6964c",
   "metadata": {},
   "source": [
    "## 0.3. Parkinson's: Not Used [Link](https://www.kaggle.com/datasets/vikasukani/parkinsons-disease-data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "parkinsons = fetch_ucirepo(id=174) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = parkinsons.data.features \n",
    "y = parkinsons.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(parkinsons.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(parkinsons.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc48c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "clean_X = X.drop(columns=[\"MDVP:Jitter\", \"MDVP:Shimmer\"]).copy()\n",
    "clean_X[\"MDVP:Jitter\"] = X[\"MDVP:Jitter\"].values.T[0]\n",
    "clean_X[\"MDVP:Shimmer\"] = X[\"MDVP:Shimmer\"].values.T[0]\n",
    "\n",
    "\n",
    "for x in clean_X.columns:\n",
    "    if clean_X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(clean_X[x])\n",
    "\n",
    "clean_y = y.copy()\n",
    "for yc in clean_y.columns:\n",
    "    if clean_y[yc].dtype == 'object':\n",
    "        clean_y[yc] = label_encoder.fit_transform(clean_y[yc])\n",
    "        \n",
    "clean_X, clean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3379aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:  # and False since we are not using parkinsons\n",
    "    clean_X.to_csv(\"./data/3-PARKINS_X.csv\")\n",
    "    clean_y.to_csv(\"./data/3-PARKINS_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f6297",
   "metadata": {},
   "source": [
    "## 0.4. Thyroid: Not Used [Link](https://www.kaggle.com/datasets/yasserhessein/thyroid-disease-data-set/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert(os.path.isfile(bridges_raw_file_name))\n",
    "thyroid_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/hypothyroid.csv\"\n",
    "df = pd.read_csv(thyroid_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"binaryClass\"])\n",
    "y = pd.DataFrame(df[\"binaryClass\"])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X[\"TBG measured\"]), set(X[\"TBG\"])  # Can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e671317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=[\"TBG measured\", \"TBG\"])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340245bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and False:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e254f7",
   "metadata": {},
   "source": [
    "## 2. Emotion [Link](https://cometa.ujaen.es/datasets/emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bbae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "emot_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/2-EMOT.csv\"\n",
    "df = pd.read_csv(emot_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df.columns[:6]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=targets)\n",
    "y = df[targets]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/2-EMOT_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/2-EMOT_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dcefe",
   "metadata": {},
   "source": [
    "## 3. Scene [Link](https://cometa.ujaen.es/datasets/scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaace8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "scene_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/3-SCENE.csv\"\n",
    "df = pd.read_csv(scene_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f3e2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('Att')]\n",
    "\n",
    "X = df[columns]\n",
    "y = df.drop(columns=columns)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_cols = dict()\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/3-SCENE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/3-SCENE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69343255",
   "metadata": {},
   "source": [
    "## 4. Flags [Link](https://cometa.ujaen.es/datasets/flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "flags_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/flags.arff\"\n",
    "assert(os.path.isfile(flags_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(flags_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5471109",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"red\", \"green\", \"blue\", \"yellow\", 'white', 'black', \"orange\"]\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/4-FLAGS_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/4-FLAGS_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fe38c",
   "metadata": {},
   "source": [
    "# 5. Foodtruck [Link](https://cometa.ujaen.es/datasets/foodtruck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "mediamill_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/foodtruck.arff\"\n",
    "assert(os.path.isfile(mediamill_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(mediamill_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['gourmet', 'snacks', 'street_food', 'italian_food',\n",
    "           'brazilian_food', 'mexican_food', 'chinese_food','japanese_food',\n",
    "           'arabic_food', 'healthy_food', 'fitness_food', 'sweets_desserts']\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_X = X.copy()\n",
    "for x in clean_X.columns:\n",
    "    if clean_X[x].dtype == 'object':\n",
    "        clean_X[x] = label_encoder.fit_transform(clean_X[x])\n",
    "\n",
    "X = clean_X.copy()\n",
    "del clean_X\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/5-FOODTRUCK_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/5-FOODTRUCK_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff6e10",
   "metadata": {},
   "source": [
    "## 6. Yeast [Link](https://cometa.ujaen.es/datasets/yeast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "yeast_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/yeast.arff\"\n",
    "assert(os.path.isfile(yeast_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(yeast_raw_file_name)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d593b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('Att')]\n",
    "\n",
    "X = df[columns]\n",
    "y = df.drop(columns=columns)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d11122",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/6-YEAST_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/6-YEAST_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5750c",
   "metadata": {},
   "source": [
    "## 7. Birds [Link](https://cometa.ujaen.es/datasets/birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04853e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "birds_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/7-BIRDS.csv\"\n",
    "df = pd.read_csv(birds_csv_file)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb84142",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[df.columns.str.startswith('A')]]  # Selecting columns with names starting with 'A'\n",
    "y = df[df.columns[df.columns.str.startswith('L')]]  # Selecting columns with names starting with 'L'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/7-BIRDS_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/7-BIRDS_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13134354",
   "metadata": {},
   "source": [
    "## 8. Genbase [Link](https://cometa.ujaen.es/datasets/genbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87187d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "genbase_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/genbase.arff\"\n",
    "assert(os.path.isfile(genbase_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(genbase_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba60513",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns[df.columns.str.startswith('PDOC')]\n",
    "\n",
    "X = df.drop(columns=columns)\n",
    "y = df[columns]\n",
    "\n",
    "for x in X.columns:\n",
    "    if X[x].dtype == 'object':\n",
    "        X[x] = label_encoder.fit_transform(X[x])\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1631308",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/8-GENBASE_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/8-GENBASE_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1da308",
   "metadata": {},
   "source": [
    "## 9. Medical [Link](https://cometa.ujaen.es/datasets/medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "medc_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/9-MEDC.csv\"\n",
    "df = pd.read_csv(medc_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70336ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=df.columns[df.columns.str.startswith('Class')])  # Selecting columns with names starting with 'L'\n",
    "y = df[df.columns[df.columns.str.startswith('Class')]]  # Selecting columns with names starting with 'A'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/9-MEDC_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/9-MEDC_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b4110",
   "metadata": {},
   "source": [
    "## 10. Enron [Link](https://cometa.ujaen.es/datasets/enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enron_csv_file = os.getcwd() + \"/data/\" + \"./raw_datasets/10-ENRON.csv\"\n",
    "df = pd.read_csv(enron_csv_file)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708392f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in df.columns if col[0].isupper()]\n",
    "\n",
    "# Create a new DataFrame with the filtered columns\n",
    "X = df.drop(columns=filtered_columns)\n",
    "y = df[filtered_columns]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_cols = dict()\n",
    "new_y_cols = dict()\n",
    "for i, col in enumerate(X.columns):\n",
    "    new_X_cols[col] = f\"Att{i+1}\"\n",
    "for i, col in enumerate(y.columns):\n",
    "    new_y_cols[col] = f\"Class{i+1}\"\n",
    "\n",
    "X = X.rename(columns=new_X_cols)\n",
    "y = y.rename(columns=new_y_cols)\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffa326",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/10-ENRON_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/10-ENRON_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012a13a",
   "metadata": {},
   "source": [
    "## 11. MediaMill [Link](https://cometa.ujaen.es/datasets/mediamill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9293698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "mediamill_raw_file_name = os.getcwd() + \"/data/\" + \"./raw_datasets/mediamill.arff\"\n",
    "assert(os.path.isfile(mediamill_raw_file_name))\n",
    "\n",
    "# Load ARFF file\n",
    "data, meta = arff.loadarff(mediamill_raw_file_name,)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, decode byte strings to regular strings if necessary\n",
    "df = df.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "df.head(), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34be7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df[df.columns[df.columns.str.startswith('Att')]]  # Selecting columns with names starting with 'A'\n",
    "y = df[df.columns[df.columns.str.startswith('Class')]]  # Selecting columns with names starting with 'L'\n",
    "\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b683b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files:\n",
    "    X.to_csv(\"./data/datasets/11-MEDIAMILL_X.csv\", index=False)\n",
    "    y.to_csv(\"./data/datasets/11-MEDIAMILL_y.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
