{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of our approach\n",
    "\n",
    "In this notebook, we include the test routine we used to compare our method, with the intention to help anyway trying to reproduce the obtained results. \n",
    "\n",
    "This notebook assumes that the datasets were cleaned using the [preprocessing notebook](./data_preprocessing.ipynb).\n",
    "The results of our run were saved to [losses.json](./.results/losses.json).\n",
    "\n",
    "If any bugs were to be caught while reading this code or while reproducing the results, please let us know on the project's [GitHub page](https://github.com/rompoggi/MCTS_ClassifierChain) via a [Pull Request](https://github.com/rompoggi/MCTS_ClassifierChain/pulls) or the [Discussions](https://github.com/rompoggi/MCTS_ClassifierChain/discussions) channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = [\n",
    "    \"2-EMOT\",\n",
    "    \"3-SCENE\",\n",
    "    \"4-FLAGS\",\n",
    "    \"5-FOODTRUCK\",\n",
    "    \"6-YEAST\",\n",
    "    \"7-BIRDS\",\n",
    "    \"8-GENBASE\",\n",
    "    # \"9-MEDC\",          # Removed as too long to test on\n",
    "    # \"10-ENRON\",        # Removed as too long to test on\n",
    "    # \"11-MEDIAMILL\",    # Removed as too long to test on\n",
    "    ]\n",
    "\n",
    "import pandas as pd\n",
    "def get_dataset(ds: str):\n",
    "    path_X = f\"./data/datasets/{ds}_X.csv\"\n",
    "    path_y = f\"./data/datasets/{ds}_y.csv\"\n",
    "    X = pd.read_csv(path_X)\n",
    "    y = pd.read_csv(path_y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss\n",
    "from datetime import datetime \n",
    "\n",
    "from mcts_inference.brute_force import brute_force as bf\n",
    "from mcts_inference.mcts import MCTS\n",
    "from mcts_inference.mc import MCC\n",
    "from mcts_inference.policy import UCB, EpsGreedy, Thompson_Sampling\n",
    "from mcts_inference.constraints import Constraint\n",
    "from mcts_inference.mcts_config import MCTSConfig\n",
    "\n",
    "def losses(X, y, chain, algo, config, loss_fns=[hamming_loss]):\n",
    "    if algo is None:\n",
    "        y_pred = chain.predict(X)\n",
    "    elif config is not None:\n",
    "        y_pred = np.abs(algo(X, chain, config))\n",
    "    else:\n",
    "        raise ValueError(\"Config cannot be None if algo is not None\")\n",
    "\n",
    "    return [fn(y, y_pred) for fn in loss_fns]\n",
    "\n",
    "def loss_algos(ds, k=5, n_repeats = 1, random_state=0, loss_fns=[hamming_loss, zero_one_loss], loss_dict={}):\n",
    "    d_time = 20.\n",
    "    n_iter = 1000\n",
    "    \n",
    "    loss_dict[ds] = {}\n",
    "    loss_dict[ds][\"PCC\"] = []\n",
    "    loss_dict[ds][\"CC\"] = []\n",
    "    loss_dict[ds][\"MCC\"] = []\n",
    "    loss_dict[ds][\"MCTS UCB(2)\"] = []\n",
    "    loss_dict[ds][\"MCTS EpsGreedy(0.2)\"] = []\n",
    "    loss_dict[ds][\"MCTS EpsGreedy(0.5)\"] = []\n",
    "    loss_dict[ds][\"MCTS Thompson_Sampling(1,1)\"] = []\n",
    "    loss_dict[ds][\"MCT1S UCB(2)\"] = []\n",
    "    loss_dict[ds][\"MCT1S EpsGreedy(0.2)\"] = []\n",
    "\n",
    "    X, y = get_dataset(ds)\n",
    "    \n",
    "    chain = ClassifierChain(SVC(max_iter=10_000, gamma=\"auto\", probability=True, random_state=random_state))\n",
    "\n",
    "    rkf = RepeatedKFold(n_splits=k, n_repeats=n_repeats, random_state=random_state)\n",
    "\n",
    "    print(f\" - {n_iter=}, {d_time=}, k={k}, n_repeats={n_repeats}\")\n",
    "    i = 1\n",
    "    for train_idx, test_idx in rkf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        print(f\"[{' '*(int(math.log(n_repeats*k,10))-int(math.log(i,10)))}{i}/{n_repeats * k} \", end=\"\")\n",
    "        if (not (y_train.nunique() > 1).all()):  # Check if there are multiple classes in the training set\n",
    "            print(f\"missing \", end=\"\")\n",
    "            # continue\n",
    "            dummy_row = pd.DataFrame({col: [0 if y_train[col].iloc[0] == 1 else 1] for col in y_train.columns}, index=[y_train.index.max() + 1])\n",
    "            y_train = pd.concat([y_train, dummy_row])\n",
    "            dummy_x_row = pd.DataFrame(np.zeros((1, len(X_train.columns))), columns=X_train.columns, index=[X_train.index.max() + 1])\n",
    "            X_train = pd.concat([X_train, dummy_x_row])\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        chain = chain.fit(X_train, y_train)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        chain = chain.fit(X_train, y_train)\n",
    "        loss = losses(X_test, y_test, chain, algo=None, config=None, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"CC\"].append(loss)   \n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], constraint=Constraint(time=True, d_time=d_time))\n",
    "        loss = losses(X_test, y_test, chain, algo=bf, config=config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"PCC\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=UCB(2), constraint=Constraint(max_iter=True, n_iter=n_iter))\n",
    "        loss = losses(X_test, y_test, chain, MCC, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCC\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=UCB(2), constraint=Constraint(max_iter=True, n_iter=n_iter))\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCTS UCB(2)\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=EpsGreedy(0.2), constraint=Constraint(max_iter=True, n_iter=n_iter))\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCTS EpsGreedy(0.2)\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=EpsGreedy(0.5), constraint=Constraint(max_iter=True, n_iter=n_iter))\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCTS EpsGreedy(0.5)\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=Thompson_Sampling(1,1), constraint=Constraint(max_iter=True, n_iter=n_iter))\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCTS Thompson_Sampling(1,1)\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=UCB(2), constraint=Constraint(max_iter=True, n_iter=n_iter), step_once=False)\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCT1S UCB(2)\"].append(loss)\n",
    "        print(f\". \", end=\"\")\n",
    "\n",
    "        config = MCTSConfig(n_classes=y.shape[1], selection_policy=EpsGreedy(0.2), constraint=Constraint(max_iter=True, n_iter=n_iter), step_once=False)\n",
    "        loss = losses(X_test, y_test, chain, MCTS, config, loss_fns=loss_fns)\n",
    "        loss_dict[ds][\"MCT1S EpsGreedy(0.2)\"].append(loss)\n",
    "        print(f\". {datetime.now().strftime('%H:%M:%S')}]\")\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    for key in loss_dict[ds].keys():\n",
    "        loss_dict[ds][key] = list(np.mean(loss_dict[ds][key], axis=0))\n",
    "\n",
    "    return loss_dict[ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, zero_one_loss\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "losses_dict = dict()\n",
    "print(\"- Hamming Loss & Zero One Loss-\")\n",
    "for ds in ds_names[:7]:\n",
    "    print(\">\"*80)\n",
    "    print(f\"> Dataset: {ds} - {datetime.now().strftime('%H:%M:%S')}\", end=\"\")\n",
    "    try:\n",
    "        L = loss_algos(ds, k=10, n_repeats=2, random_state=0, loss_fns=[hamming_loss, zero_one_loss], loss_dict=losses_dict)\n",
    "        print(\"_\"*32)\n",
    "        print(f\"Brute Force (PCC)           : {L['PCC']}, Score: {[1- x for x in L['PCC']]}\")\n",
    "        print(f\"Classifier Chain (CC)       : {L['CC']}, Score: {[1- x for x in L['CC']]}\")\n",
    "        print(f\"Monte Carlo CC (MCC)        : {L['MCC']}, Score: {[1- x for x in L['MCC']]}\")\n",
    "        print(f\"MCTS UCB(2)                 : {L['MCTS UCB(2)']}, Score: {[1- x for x in L['MCTS UCB(2)']]}\")\n",
    "        print(f\"MCTS EpsGreedy(0.2)         : {L['MCTS EpsGreedy(0.2)']}, Score: {[1-x for x in L['MCTS EpsGreedy(0.2)']]}\")\n",
    "        print(f\"MCTS EpsGreedy(0.5)         : {L['MCTS EpsGreedy(0.5)']}, Score: {[1-x for x in L['MCTS EpsGreedy(0.5)']]}\")\n",
    "        print(f\"MCTS Thompson_Sampling(1,1) : {L['MCTS Thompson_Sampling(1,1)']}, Score: {[1-x for x in L['MCTS Thompson_Sampling(1,1)']]}\")\n",
    "        print(f\"MCT1S UCB(2)                : {L['MCT1S UCB(2)']}, Score: {[1- x for x in L['MCT1S UCB(2)']]}\")\n",
    "        print(f\"MCT1S EpsGreedy(0.2)        : {L['MCT1S EpsGreedy(0.2)']}, Score: {[1- x for x in L['MCT1S EpsGreedy(0.2)']]}\")\n",
    "        print(\"_\"*32,\"\\n\")\n",
    "\n",
    "        with open('./data/.results/temp_losses.json', 'a') as outfile:  # Save results to file\n",
    "            L[\"datetime\"] = datetime.now().strftime('%H:%M:%S')\n",
    "            json.dump(L, outfile)\n",
    "            del L[\"datetime\"]  # Remove this key as we do not save it if the whole test goes through\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {ds}: {e}\")\n",
    "        losses_dict[ds] = None\n",
    "\n",
    "losses_dict[\"datetime\"] = datetime.now().strftime('%H:%M:%S')\n",
    "with open('./data/.results/losses.json', 'a') as outfile:  # Save all results\n",
    "    json.dump(losses_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = [\n",
    "    \"2-EMOT\",\n",
    "    \"3-SCENE\",\n",
    "    \"4-FLAGS\",\n",
    "    \"5-FOODTRUCK\",\n",
    "    \"6-YEAST\",\n",
    "    \"7-BIRDS\",\n",
    "    \"8-GENBASE\",\n",
    "    \"9-MEDC\",\n",
    "    \"10-ENRON\",\n",
    "    \"11-MEDIAMILL\",\n",
    "    ]\n",
    "\n",
    "ds_names[:7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
